{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbe01333",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62fe36a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nLarge language models are powerful deep learning models that use large amounts of text data to generate language that is similar in structure and meaning to the data it is trained on.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "llm(\"explain large language models in one sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "342330ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import(\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d65c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model_name=\"gpt-4\",temperature=0.3)\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an expert data scientist\"),\n",
    "    HumanMessage(content=\"Write a python script that trains a neural network on simulated data\")\n",
    "]\n",
    "response=chat(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e560961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here is a simple example of a Python script that uses the Keras library to train a neural network on simulated data. This script generates a dataset for a binary classification problem, then trains a neural network on it.\n",
      "\n",
      "```python\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from sklearn.datasets import make_classification\n",
      "from sklearn.model_selection import train_test_split\n",
      "import numpy as np\n",
      "\n",
      "# Set a random seed for reproducibility\n",
      "np.random.seed(0)\n",
      "\n",
      "# Generate a binary classification dataset\n",
      "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
      "\n",
      "# Split the dataset into training set and test set\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
      "\n",
      "# Define the neural network model\n",
      "model = Sequential()\n",
      "model.add(Dense(10, input_dim=20, activation='relu'))\n",
      "model.add(Dense(1, activation='sigmoid'))\n",
      "\n",
      "# Compile the model\n",
      "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "\n",
      "# Train the model\n",
      "model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0)\n",
      "\n",
      "# Evaluate the model\n",
      "_, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "```\n",
      "\n",
      "This script first generates a dataset with 1000 samples, 20 features, and a binary target variable. It then splits this dataset into a training set and a test set. The neural network model is defined with one hidden layer with 10 nodes and an output layer with 1 node (for the binary target variable). The model is then compiled and trained on the training data. Finally, the model's accuracy is evaluated on the test data.\n"
     ]
    }
   ],
   "source": [
    "print(response.content,end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcfcc85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert data scientist with an expertise in building deep learning models.\n",
    "Explain the concept of {concept} in a couple of lines\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"concept\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1750d6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['concept'], output_parser=None, partial_variables={}, template='\\nYou are an expert data scientist with an expertise in building deep learning models.\\nExplain the concept of {concept} in a couple of lines\\n', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71d37623",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRegularization is a technique used to reduce the complexity of a machine learning model by penalizing its weights. It helps reduce overfitting by adding a penalty to the loss function during training, which encourages the model to learn simpler, more generalizable weights. This helps the model generalize better to unseen data and reduce the risk of memorizing the training data.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(prompt.format(concept=\"regularization\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31f2b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert data scientist with an expertise in a building deep learning models.\n",
    "Explain the concept of {concept}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"concept\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff9335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd368ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(prompt.format(concept=\"autoencoder\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
